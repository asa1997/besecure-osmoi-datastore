[
  {
    "type": "model",
    "name": "MathCoder",
    "description": "MathCoder is a family of models capable of generating code-based solutions for solving challenging math problems.",
    "created_date": {
      "explanation": "",
      "value": "2023-10-05"
    },
    "organization": "Shanghai AI Laboratory",
    "url": "https://arxiv.org/pdf/2310.03731.pdf",
    "model_card": "none",
    "modality": "text; text",
    "analysis": "Evaluated on GSM8K and the competition-level MATH dataset.",
    "size": "70B parameters (dense)",
    "dependencies": ["GPT-4", "LLaMA 2"],
    "training_emissions": "unknown",
    "training_time": "unknown",
    "training_hardware": "32 NVIDIA A800 80GB GPUs",
    "quality_control": "none",
    "access": "open",
    "license": {
      "explanation": "",
      "value": "unknown"
    },
    "intended_uses": "bridging the gap between natural language understanding and computational problem-solving",
    "prohibited_uses": "none",
    "monitoring": "none",
    "feedback": "none"
  },
  {
    "type": "model",
    "name": "Falcon-180B",
    "organization": "UAE Technology Innovation Institute",
    "description": "Falcon-180B is a 180B parameters causal decoder-only model built by TII and trained on 3,500B tokens of RefinedWeb enhanced with curated corpora.",
    "created_date": "2023-09-06T00:00:00.000Z",
    "url": "https://falconllm.tii.ae/falcon-models.html",
    "model_card": "https://huggingface.co/tiiuae/falcon-180B",
    "modality": "text; text",
    "analysis": "Falcon-180B outperforms LLaMA-2, StableLM, RedPajama, MPT on the Open LLM Leaderboard at https://huggingface.co/spaces/HuggingFaceH4/open_llm_leaderboard.",
    "size": "180B parameters (dense)",
    "dependencies": [
      "RefinedWeb"
    ],
    "training_emissions": "",
    "training_time": "9 months",
    "training_hardware": "4096 A100 40GB GPUs",
    "quality_control": "",
    "access": "open",
    "license": "unknown",
    "intended_uses": "",
    "prohibited_uses": "Production use without adequate assessment of risks and mitigation; any use cases which may be considered irresponsible or harmful.",
    "monitoring": "None",
    "feedback": "https://huggingface.co/tiiuae/falcon-180b/discussions"
  },
  {
    "type": "model",
    "name": "StarCoder",
    "organization": "BigCode",
    "description": "StarCoder is a Large Language Model for Code (Code LLM) trained on permissively licensed data from GitHub, including from 80+ programming languages, Git commits, GitHub issues, and Jupyter notebooks.",
    "created_date": "2023-05-09T00:00:00.000Z",
    "url": "https://arxiv.org/pdf/2305.06161.pdf",
    "model_card": "https://huggingface.co/bigcode/starcoder",
    "modality": "code; code",
    "analysis": "Tested on several benchmarks, most notably Python benchmark HumanEval.",
    "size": "15.5B parameters (dense)",
    "dependencies": [
      "The Stack"
    ],
    "training_emissions": "16.68 tons of CO2eq",
    "training_time": "2 days",
    "training_hardware": "64 NVIDIA A100 GPUs",
    "quality_control": "No specific quality control is mentioned in model training, though details on data processing and how the tokenizer was trained are provided in the paper.",
    "access": "open",
    "license": "Apache 2.0",
    "intended_uses": "With a Tech Assistant prompt and not as an instruction model given training limitations.",
    "prohibited_uses": "",
    "monitoring": "",
    "feedback": "https://huggingface.co/bigcode/starcoder/discussions"
  },
  {
    "type": "model",
    "name": "Flan-UL2",
    "organization": "Google",
    "description": "",
    "created_date": "2023-03-02T00:00:00.000Z",
    "url": "https://arxiv.org/abs/2205.05131",
    "model_card": "",
    "modality": "text; text",
    "analysis": "",
    "size": "20B parameters (dense)",
    "dependencies": [
      "UL2",
      "Flan Collection"
    ],
    "training_emissions": "",
    "training_time": "",
    "training_hardware": "",
    "quality_control": "",
    "access": "open",
    "license": "Apache 2.0",
    "intended_uses": "",
    "prohibited_uses": "",
    "monitoring": "",
    "feedback": ""
  }
]
